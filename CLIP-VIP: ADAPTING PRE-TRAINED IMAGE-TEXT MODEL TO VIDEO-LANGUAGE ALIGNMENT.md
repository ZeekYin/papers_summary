CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment

The paper tackles the challenge of adapting image-text pre-trained models like CLIP to video-text pre-training and downstream tasks.
The paper proposes CLIP-ViP, which consists of an Omnisource Cross-modal Learning method and a Video Proxy mechanism.
The paper shows that CLIP-ViP improves the performance of CLIP on video-text retrieval by a large margin and achieves state-of-the-art results on four benchmarks. Omnisource Cross-modal Learning
The paper introduces auxiliary captions generated by an image captioning model to reduce the language domain gap between video-subtitle data and downstream data.
The paper studies different variants of cross-modal learning losses to jointly learn from video-subtitle and frame-caption pairs.
The paper finds that LV$S;C+LF$C performs best, which expands the number of negative pairs in contrastive learning. Video Proxy Mechanism
The paper introduces video proxy tokens to enable the Vision Transformer to process both images and videos with minimal modification.
The paper designs a proxy-guided attention mechanism that allows video proxy tokens to interact with all tokens, while patch tokens only interact with video proxy tokens and patch tokens within the same frame.
The paper demonstrates that the video proxy mechanism can better model temporal information and reduce conflicts with the pre-trained CLIP model.


